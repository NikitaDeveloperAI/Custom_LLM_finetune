{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bd744f-c31c-44e5-9ed8-ff49ca2f31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1923516-bfc2-4f9f-8a64-60cafe6b7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "SYSTEM_MESSAGE = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c9ac7f-961e-485b-b2ed-215384c8a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"parquet\", data_files=\"./dataset/spider-train.parquet\", split=\"train\")\n",
    "test_dataset = load_dataset(\"parquet\", data_files=\"./dataset/spider-test.parquet\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe1a129-100f-4b19-a8d2-4906a75cbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = \"\"\"CREATE TABLE table_name_83 (bleeding_time VARCHAR, platelet_count VARCHAR, condition VARCHAR)\"\"\"\n",
    "\n",
    "def preprocess_conversations(sample: str) -> str:\n",
    "    return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": SYSTEM_MESSAGE.format(schema=SCHEMA)},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"query\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_conversations, remove_columns=train_dataset.features, batched=False)\n",
    "test_dataset = test_dataset.map(preprocess_conversations, remove_columns=test_dataset.features, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02f6e5a-5416-4878-bd71-3b8397ebfd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # or `codellama/CodeLlama-7b-hf` or `mistralai/Mistral-7B-v0.1`\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c9617d-d39e-443c-a26a-2536df422795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llm-text-to-sql\",              # directory to save and repository id\n",
    "    num_train_epochs=10,                    # number of training epochs\n",
    "    max_steps=300,\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=6,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                        # log every n steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                      # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffab1af8-5370-477f-9967-dbad05c991fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NSKZ\\PycharmProjects\\PersonalLLM\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "C:\\Users\\NSKZ\\PycharmProjects\\PersonalLLM\\venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:342: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,  # the rank of the LoRA matrices\n",
    "    lora_alpha=128, # the weight\n",
    "    lora_dropout=0.1, # dropout to add to the LoRA layers\n",
    "    bias=\"none\", # add bias to the nn.Linear layers?\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"], # the name of the layers to add LoRA\n",
    "    modules_to_save=None, # layers to unfreeze and train from the original pre-trained model\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc90c42-b098-4e41-b72f-f4e82c6bcbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\NSKZ\\PycharmProjects\\PersonalLLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\NSKZ\\PycharmProjects\\PersonalLLM\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:670: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c448ad3-a796-49b6-984e-fe1e0b22ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained('llm-text-to-sql', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdeb2b-0cce-4ec1-9c83-347ec2306bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6d41a-eea9-41ff-ab4a-d16dd1b24d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# # Load PEFT model on CPU\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     args.output_dir,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )\n",
    "# # Merge LoRA and base model and save\n",
    "# merged_model = model.merge_and_unload()\n",
    "# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9aafb7-95fb-414b-a961-8aa3960cb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "# Test on sample\n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, \n",
    "               max_new_tokens=256, \n",
    "               do_sample=True, \n",
    "               temperature=0.1, \n",
    "               top_k=50, \n",
    "               top_p=0.1, \n",
    "               eos_token_id=pipe.tokenizer.eos_token_id, \n",
    "               pad_token_id=pipe.tokenizer.pad_token_id,\n",
    "              )\n",
    "\n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b7618-c2c3-4780-b451-c040c4d6c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(sample):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "success_rate = []\n",
    "number_of_eval_samples = 10\n",
    "# iterate over eval dataset and predict\n",
    "for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n",
    "    success_rate.append(evaluate(s))\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667d77f-5d57-45f8-b658-7564741a4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import requests as r\n",
    "# from transformers import AutoTokenizer\n",
    "# from datasets import load_dataset\n",
    "# from random import randint\n",
    "\n",
    "# # Load our test dataset and Tokenizer again\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"code-llama-7b-text-to-sql\")\n",
    "# eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "# rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "# # generate the same prompt as for the first local test\n",
    "# prompt = tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "# request= {\"inputs\":prompt,\"parameters\":{\"temperature\":0.2, \"top_p\": 0.95, \"max_new_tokens\": 256}}\n",
    "\n",
    "# # send request to inference server\n",
    "# resp = r.post(\"http://127.0.0.1:8080/generate\", json=request)\n",
    "\n",
    "# output = resp.json()[\"generated_text\"].strip()\n",
    "# time_per_token = resp.headers.get(\"x-time-per-token\")\n",
    "# time_prompt_tokens = resp.headers.get(\"x-prompt-tokens\")\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "# print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "# print(f\"Generated Answer:\\n{output}\")\n",
    "# print(f\"Latency per token: {time_per_token}ms\")\n",
    "# print(f\"Latency prompt encoding: {time_prompt_tokens}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b0e08-7961-4fbc-b6d0-6d8a65172497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker stop tgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ad711-506a-4075-820c-243a3422261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{pipe.tokenizer.eos_token_id=}\")\n",
    "print(f\"{pipe.tokenizer.pad_token_id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35abdd-b5dc-48a3-8c46-6d8c3db74aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = load_dataset(\"parquet\", data_files=\"dataset/spider-train.parquet\", split=\"train\")\n",
    "data.rename_column('question', 'question2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428a33d-29d6-4c3d-947f-0c251a18ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d7f1c-8f91-43f7-a126-27ebc5dd358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f2ef3-7c7e-4b05-865f-916b3144e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rename_column('question', 'question2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769944a-5f8e-4571-ba35-f6125bed0680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
