{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee1e619-1258-44b0-8e70-b4c0153f79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda'\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 32\n",
    "max_iterations = 100000\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 250\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed83350-3bbe-42b1-addf-129cf23ed250",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "vocabulary_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2c41d4-d7fe-4d35-9928-2f4651835687",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "encode_function = lambda s: [string_to_int[c] for c in s]\n",
    "decode_function = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode_function(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8223d6ac-d9ba-4324-9fc7-395deff6ca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[69, 62, 13, 13, 58, 69, 69,  1],\n",
      "        [65, 66, 60, 65,  1, 82, 72, 78],\n",
      "        [77, 62, 75, 70, 76,  1, 72, 63],\n",
      "        [77, 65, 62,  1, 70, 58, 66, 61],\n",
      "        [77, 58, 77, 62, 76,  1, 80, 65],\n",
      "        [58, 64, 62, 12,  1, 63, 72, 75],\n",
      "        [32, 72, 75, 72, 77, 65, 82, 14],\n",
      "        [62,  1, 48, 66, 71,  1, 51, 72],\n",
      "        [ 0,  0,  3, 42, 72,  1, 72, 71],\n",
      "        [61,  1, 76, 65, 62,  1, 80, 58],\n",
      "        [ 1, 61, 62, 73, 62, 71, 61, 62],\n",
      "        [ 1, 77, 65, 58, 71,  1, 77, 65],\n",
      "        [82, 62, 75,  0, 58, 70, 72, 71],\n",
      "        [71, 78, 62,  1, 47, 62, 75, 79],\n",
      "        [63, 62, 60, 77,  1, 82, 72, 78],\n",
      "        [65, 62, 75, 62,  0, 80, 58, 76],\n",
      "        [76, 62, 27,  1, 65, 72, 80,  1],\n",
      "        [ 0, 60, 72, 70, 73, 69, 82,  1],\n",
      "        [12,  1, 64, 75, 58, 79, 62, 69],\n",
      "        [58, 80, 58, 82,  1, 72, 75,  1],\n",
      "        [62,  0, 73, 75, 72, 61, 78, 60],\n",
      "        [66, 64, 62, 75,  1, 58, 71, 61],\n",
      "        [58, 77, 62, 79, 62, 75,  1, 73],\n",
      "        [ 1, 58, 60, 68, 71, 72, 80, 69],\n",
      "        [70, 58, 69,  1, 59, 62,  1, 64],\n",
      "        [ 1, 72, 63,  1, 62, 74, 78, 66],\n",
      "        [ 1, 80, 66, 77, 65, 72, 78, 77],\n",
      "        [62,  1, 63, 72, 75,  1, 64, 62],\n",
      "        [13, 65, 72, 75, 76, 62,  1, 58],\n",
      "        [ 1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [42, 35,  1, 30, 49, 48,  1, 42],\n",
      "        [ 1, 76, 65, 72, 80,  1, 77, 65]], device='cuda:0')\n",
      "targets: tensor([[62, 13, 13, 58, 69, 69,  1, 64],\n",
      "        [66, 60, 65,  1, 82, 72, 78,  1],\n",
      "        [62, 75, 70, 76,  1, 72, 63,  1],\n",
      "        [65, 62,  1, 70, 58, 66, 61,  1],\n",
      "        [58, 77, 62, 76,  1, 80, 65, 62],\n",
      "        [64, 62, 12,  1, 63, 72, 75,  1],\n",
      "        [72, 75, 72, 77, 65, 82, 14,  0],\n",
      "        [ 1, 48, 66, 71,  1, 51, 72, 72],\n",
      "        [ 0,  3, 42, 72,  1, 72, 71, 62],\n",
      "        [ 1, 76, 65, 62,  1, 80, 58, 76],\n",
      "        [61, 62, 73, 62, 71, 61, 62, 61],\n",
      "        [77, 65, 58, 71,  1, 77, 65, 62],\n",
      "        [62, 75,  0, 58, 70, 72, 71, 64],\n",
      "        [78, 62,  1, 47, 62, 75, 79, 66],\n",
      "        [62, 60, 77,  1, 82, 72, 78,  1],\n",
      "        [62, 75, 62,  0, 80, 58, 76,  1],\n",
      "        [62, 27,  1, 65, 72, 80,  1, 62],\n",
      "        [60, 72, 70, 73, 69, 82,  1, 80],\n",
      "        [ 1, 64, 75, 58, 79, 62, 69, 82],\n",
      "        [80, 58, 82,  1, 72, 75,  1, 75],\n",
      "        [ 0, 73, 75, 72, 61, 78, 60, 77],\n",
      "        [64, 62, 75,  1, 58, 71, 61,  1],\n",
      "        [77, 62, 79, 62, 75,  1, 73, 69],\n",
      "        [58, 60, 68, 71, 72, 80, 69, 62],\n",
      "        [58, 69,  1, 59, 62,  1, 64, 78],\n",
      "        [72, 63,  1, 62, 74, 78, 66, 73],\n",
      "        [80, 66, 77, 65, 72, 78, 77,  1],\n",
      "        [ 1, 63, 72, 75,  1, 64, 62, 71],\n",
      "        [65, 72, 75, 76, 62,  1, 58, 77],\n",
      "        [ 1,  1,  1,  1,  1,  1,  1, 60],\n",
      "        [35,  1, 30, 49, 48,  1, 42, 43],\n",
      "        [76, 65, 72, 80,  1, 77, 65, 62]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_portion = 0.8\n",
    "cutoff_index = int(train_portion * len(data))\n",
    "train_data = data[:cutoff_index]\n",
    "val_data = data[cutoff_index:]\n",
    "\n",
    "\n",
    "def get_batch(split: str) -> Union[torch.Tensor, torch.Tensor]:\n",
    "    data = train_data if split != 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Union[torch.Tensor, torch.Tensor]:\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index: torch.Tensor, max_new_tokens: int) -> int:\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocabulary_size)\n",
    "m = model.to(device)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode_function(m.generate(context, max_new_tokens=500)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae2351-e820-46e1-9eeb-dcdb8b601205",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4aada00-9a60-4acc-89fe-3b828325a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0448431968688965\n",
      "4.879584789276123\n",
      "4.867735385894775\n",
      "4.880375862121582\n",
      "4.7913689613342285\n",
      "4.766678810119629\n",
      "4.799264907836914\n",
      "4.72320032119751\n",
      "4.715097904205322\n",
      "4.746890068054199\n",
      "4.540067195892334\n",
      "4.578786849975586\n",
      "4.491811275482178\n",
      "4.561199188232422\n",
      "4.5131425857543945\n",
      "4.5525126457214355\n",
      "4.448546886444092\n",
      "4.475431442260742\n",
      "4.427471160888672\n",
      "4.397488594055176\n",
      "4.422561168670654\n",
      "4.496092319488525\n",
      "4.292932987213135\n",
      "4.296186447143555\n",
      "4.232193470001221\n",
      "4.326360702514648\n",
      "4.1308441162109375\n",
      "4.302886962890625\n",
      "4.132615089416504\n",
      "4.131889343261719\n",
      "4.166206359863281\n",
      "4.045754432678223\n",
      "3.9650299549102783\n",
      "3.972799777984619\n",
      "4.066009998321533\n",
      "3.9316325187683105\n",
      "3.9750003814697266\n",
      "3.885469436645508\n",
      "3.898817777633667\n",
      "3.8667001724243164\n",
      "3.8263702392578125\n",
      "3.7711286544799805\n",
      "3.8882060050964355\n",
      "3.8513224124908447\n",
      "3.8290200233459473\n",
      "3.815999984741211\n",
      "3.835566520690918\n",
      "3.6268458366394043\n",
      "3.6418404579162598\n",
      "3.631218671798706\n",
      "3.662074327468872\n",
      "3.654494285583496\n",
      "3.618695020675659\n",
      "3.725912094116211\n",
      "3.5238637924194336\n",
      "3.5787036418914795\n",
      "3.6003243923187256\n",
      "3.5113813877105713\n",
      "3.556448459625244\n",
      "3.4487216472625732\n",
      "3.491607904434204\n",
      "3.4306905269622803\n",
      "3.4894843101501465\n",
      "3.3729166984558105\n",
      "3.4727673530578613\n",
      "3.4500160217285156\n",
      "3.2652525901794434\n",
      "3.329146385192871\n",
      "3.304502487182617\n",
      "3.288010835647583\n",
      "3.30405855178833\n",
      "3.259232759475708\n",
      "3.277484655380249\n",
      "3.3347666263580322\n",
      "3.1964001655578613\n",
      "3.23164439201355\n",
      "3.1914761066436768\n",
      "3.2984232902526855\n",
      "3.3779361248016357\n",
      "3.196856737136841\n",
      "3.0889413356781006\n",
      "3.1637492179870605\n",
      "3.092421531677246\n",
      "3.137446880340576\n",
      "3.1536407470703125\n",
      "3.2077367305755615\n",
      "3.1680045127868652\n",
      "3.186436891555786\n",
      "3.2596826553344727\n",
      "3.0615415573120117\n",
      "3.019430637359619\n",
      "3.0760579109191895\n",
      "3.0728790760040283\n",
      "3.0159873962402344\n",
      "2.925647497177124\n",
      "3.063530206680298\n",
      "3.0038716793060303\n",
      "2.958066940307617\n",
      "2.940026044845581\n",
      "2.9699573516845703\n",
      "2.9310250282287598\n",
      "3.006324529647827\n",
      "2.8210248947143555\n",
      "2.886018753051758\n",
      "2.8160526752471924\n",
      "2.878106117248535\n",
      "2.8287510871887207\n",
      "2.8151004314422607\n",
      "2.910641670227051\n",
      "2.797229051589966\n",
      "2.9758522510528564\n",
      "2.9560399055480957\n",
      "2.8073890209198\n",
      "2.794668197631836\n",
      "2.8290412425994873\n",
      "2.8474063873291016\n",
      "2.9171853065490723\n",
      "2.8576345443725586\n",
      "2.8499386310577393\n",
      "2.828216075897217\n",
      "2.8602442741394043\n",
      "2.8797354698181152\n",
      "2.665351390838623\n",
      "2.9547319412231445\n",
      "2.7131972312927246\n",
      "2.758233070373535\n",
      "2.9059267044067383\n",
      "2.7470686435699463\n",
      "2.6574673652648926\n",
      "2.66723370552063\n",
      "2.723698616027832\n",
      "2.6110570430755615\n",
      "2.7687978744506836\n",
      "2.6662681102752686\n",
      "2.7371997833251953\n",
      "2.717715263366699\n",
      "2.712177276611328\n",
      "2.697500228881836\n",
      "2.7186317443847656\n",
      "2.8607757091522217\n",
      "2.6871321201324463\n",
      "2.898305892944336\n",
      "2.5293619632720947\n",
      "2.7156925201416016\n",
      "2.487407922744751\n",
      "2.6138038635253906\n",
      "2.6512093544006348\n",
      "2.7788779735565186\n",
      "2.640388250350952\n",
      "2.6836097240448\n",
      "2.794041872024536\n",
      "2.5965142250061035\n",
      "2.648467779159546\n",
      "2.638272523880005\n",
      "2.675785541534424\n",
      "2.6279261112213135\n",
      "2.6601150035858154\n",
      "2.4976603984832764\n",
      "2.629598617553711\n",
      "2.6990411281585693\n",
      "2.6563971042633057\n",
      "2.6665003299713135\n",
      "2.6882317066192627\n",
      "2.5668838024139404\n",
      "2.528700828552246\n",
      "2.631397008895874\n",
      "2.621199131011963\n",
      "2.7072105407714844\n",
      "2.639972686767578\n",
      "2.5345945358276367\n",
      "2.7219595909118652\n",
      "2.6159398555755615\n",
      "2.6613261699676514\n",
      "2.660367488861084\n",
      "2.5209755897521973\n",
      "2.4494788646698\n",
      "2.3950681686401367\n",
      "2.4848735332489014\n",
      "2.5550456047058105\n",
      "2.64235258102417\n",
      "2.5921409130096436\n",
      "2.654639482498169\n",
      "2.6239147186279297\n",
      "2.609165668487549\n",
      "2.496654987335205\n",
      "2.722242832183838\n",
      "2.5567777156829834\n",
      "2.6221625804901123\n",
      "2.694045066833496\n",
      "2.534139633178711\n",
      "2.586907148361206\n",
      "2.674851417541504\n",
      "2.569089651107788\n",
      "2.4674971103668213\n",
      "2.523909568786621\n",
      "2.509225368499756\n",
      "2.6823089122772217\n",
      "2.633335828781128\n",
      "2.5491201877593994\n",
      "2.6140708923339844\n",
      "2.541032314300537\n",
      "2.7248425483703613\n",
      "2.634242057800293\n",
      "2.5204713344573975\n",
      "2.6958155632019043\n",
      "2.629275321960449\n",
      "2.5367271900177\n",
      "2.604928731918335\n",
      "2.5145366191864014\n",
      "2.6098358631134033\n",
      "2.5630743503570557\n",
      "2.599118232727051\n",
      "2.6278889179229736\n",
      "2.5012950897216797\n",
      "2.5940310955047607\n",
      "2.4713099002838135\n",
      "2.6541152000427246\n",
      "2.540283203125\n",
      "2.580219030380249\n",
      "2.4863169193267822\n",
      "2.515800952911377\n",
      "2.52163028717041\n",
      "2.5250885486602783\n",
      "2.578420877456665\n",
      "2.4878158569335938\n",
      "2.4370787143707275\n",
      "2.613170862197876\n",
      "2.463589668273926\n",
      "2.39624285697937\n",
      "2.5437850952148438\n",
      "2.6751585006713867\n",
      "2.48445200920105\n",
      "2.479614019393921\n",
      "2.7135813236236572\n",
      "2.5650863647460938\n",
      "2.550017833709717\n",
      "2.48593807220459\n",
      "2.5878477096557617\n",
      "2.6751537322998047\n",
      "2.5496578216552734\n",
      "2.4668691158294678\n",
      "2.691567897796631\n",
      "2.6792030334472656\n",
      "2.580984592437744\n",
      "2.5877726078033447\n",
      "2.5075035095214844\n",
      "2.5122742652893066\n",
      "2.4053661823272705\n",
      "2.6447699069976807\n",
      "2.4587838649749756\n",
      "2.5464131832122803\n",
      "2.491152048110962\n",
      "2.6569302082061768\n",
      "2.553182601928711\n",
      "2.5797977447509766\n",
      "2.6948494911193848\n",
      "2.449941396713257\n",
      "2.626258134841919\n",
      "2.4010009765625\n",
      "2.435051918029785\n",
      "2.4036471843719482\n",
      "2.5464928150177\n",
      "2.429229259490967\n",
      "2.593414783477783\n",
      "2.437844753265381\n",
      "2.530050277709961\n",
      "2.5388596057891846\n",
      "2.5729315280914307\n",
      "2.4118974208831787\n",
      "2.553192138671875\n",
      "2.5506021976470947\n",
      "2.5211100578308105\n",
      "2.5503220558166504\n",
      "2.5068178176879883\n",
      "2.5779809951782227\n",
      "2.545710563659668\n",
      "2.3782522678375244\n",
      "2.4776418209075928\n",
      "2.4789721965789795\n",
      "2.503973960876465\n",
      "2.558765172958374\n",
      "2.591329336166382\n",
      "2.380997896194458\n",
      "2.484131097793579\n",
      "2.460916042327881\n",
      "2.495835542678833\n",
      "2.6268270015716553\n",
      "2.4193172454833984\n",
      "2.6887974739074707\n",
      "2.6891415119171143\n",
      "2.6648898124694824\n",
      "2.6296579837799072\n",
      "2.4914045333862305\n",
      "2.438676118850708\n",
      "2.410402536392212\n",
      "2.523939371109009\n",
      "2.554126262664795\n",
      "2.481581449508667\n",
      "2.5100314617156982\n",
      "2.6079928874969482\n",
      "2.60483717918396\n",
      "2.641392707824707\n",
      "2.4943339824676514\n",
      "2.3677101135253906\n",
      "2.4116528034210205\n",
      "2.473940134048462\n",
      "2.500457525253296\n",
      "2.4476537704467773\n",
      "2.6241202354431152\n",
      "2.4488911628723145\n",
      "2.558255672454834\n",
      "2.4822092056274414\n",
      "2.4879684448242188\n",
      "2.4012250900268555\n",
      "2.546842575073242\n",
      "2.5601396560668945\n",
      "2.439678430557251\n",
      "2.4801878929138184\n",
      "2.498340368270874\n",
      "2.556018114089966\n",
      "2.530557632446289\n",
      "2.5632455348968506\n",
      "2.4599967002868652\n",
      "2.4150171279907227\n",
      "2.5333731174468994\n",
      "2.5696423053741455\n",
      "2.463989496231079\n",
      "2.5904998779296875\n",
      "2.527841567993164\n",
      "2.4130825996398926\n",
      "2.560776710510254\n",
      "2.522465229034424\n",
      "2.512392997741699\n",
      "2.513568878173828\n",
      "2.6454880237579346\n",
      "2.5294647216796875\n",
      "2.538534641265869\n",
      "2.532470703125\n",
      "2.5305607318878174\n",
      "2.4683010578155518\n",
      "2.4563140869140625\n",
      "2.4874677658081055\n",
      "2.529665470123291\n",
      "2.547523021697998\n",
      "2.57411789894104\n",
      "2.436281681060791\n",
      "2.616002321243286\n",
      "2.447490692138672\n",
      "2.4822046756744385\n",
      "2.5937061309814453\n",
      "2.549849510192871\n",
      "2.5795464515686035\n",
      "2.457421064376831\n",
      "2.6801817417144775\n",
      "2.394078493118286\n",
      "2.5316882133483887\n",
      "2.4821553230285645\n",
      "2.4502453804016113\n",
      "2.449885606765747\n",
      "2.44756817817688\n",
      "2.4391679763793945\n",
      "2.450308084487915\n",
      "2.4983060359954834\n",
      "2.5327305793762207\n",
      "2.478278875350952\n",
      "2.6017367839813232\n",
      "2.501821756362915\n",
      "2.483022689819336\n",
      "2.3997983932495117\n",
      "2.5931813716888428\n",
      "2.4882404804229736\n",
      "2.450701951980591\n",
      "2.3907363414764404\n",
      "2.502559185028076\n",
      "2.412869691848755\n",
      "2.665874481201172\n",
      "2.4995322227478027\n",
      "2.554673910140991\n",
      "2.498072624206543\n",
      "2.5323874950408936\n",
      "2.390613317489624\n",
      "2.439556121826172\n",
      "2.5119783878326416\n",
      "2.410341739654541\n",
      "2.4883785247802734\n",
      "2.438264846801758\n",
      "2.578951358795166\n",
      "2.319237470626831\n",
      "2.4581925868988037\n",
      "2.2623181343078613\n",
      "2.4205002784729004\n",
      "2.3509116172790527\n",
      "2.559340476989746\n",
      "2.5464096069335938\n",
      "2.4918956756591797\n",
      "2.525418758392334\n",
      "2.531977891921997\n",
      "2.419226884841919\n",
      "2.4973227977752686\n",
      "2.4585890769958496\n",
      "2.450996160507202\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iteration {i}, train loss {losses['train']}, val loss {losses['val']}\")\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e9b0f1-ef8f-4dfb-8ee1-717a4fd85b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wan out asathithebot h he an tureo  thectiles, IL. alded 90Q,\"o ine; trkecate yorofe\n",
      "ADon SE a, s ashexigintof, ornbeathin s.\n",
      "\n",
      "5IOn ans Prerked  grm titinous rggenongr g™ t wheainde Dend ises Thite s tran'sautod hendrnd ge e s paly—ande leligr wimy e awind lereitr ct \"io wourgles,00. d whexe rgu ct I t bent, a sengo ome k he thoovict t asthabyr igr beroran ond t her berous orecer yo piote thetive rrior d sinowa\n",
      "\n",
      "H\n",
      "\n",
      "tre The t anthe\n",
      "L, t y.\n",
      "osoung t t ofth Ring™ earepoous ctich hy jesorg don   es \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode_function(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
